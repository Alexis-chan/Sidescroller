<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Dico IA</title>
  <script src="js/jquery-3.7.0.min.js"></script>    
  
  <script src="js/marked.umd.js"></script>
  <script src="js/index.umd.js"></script>
  <link href="css/chuta.css" rel="stylesheet" type="text/css" />
    <link href="css/style_page.css" rel="stylesheet" type="text/css" />

    <script>
      // ‚ö†Ô∏è  notez le .gfmHeadingId apr√®s markedGfmHeadingId
      /* marked.use(markedGfmHeadingId.gfmHeadingId({ prefix: "dico-" })); */
      marked.use(markedGfmHeadingId.gfmHeadingId());

      document.addEventListener("DOMContentLoaded", () => {
        const md = document.getElementById("markdown").value.trim();
        document.getElementById("content").innerHTML = marked.parse(md);
      });
    </script>

<style>
main{
    /* plus besoin de flex du tout */
    max-width: 1200px;
    margin-inline: auto;    /* centre le conteneur */
    padding-inline: 1rem;
}

h1 {
  font-family: monospace;
  border: solid lightgray 2px;
  border-radius: 2rem;
  text-align: center;
}


html{ scroll-behavior:smooth; }
h2,h3{ scroll-margin-top:1rem; }

img:hover{
    transform: scale(1.75);
}

main div.content a {
    text-decoration: none !important;
    background: deepskyblue !important;
    padding: 4px 8px !important;
    color: white !important;
    letter-spacing: 0.05em !important;
    font-weight: 600 !important;
    border-radius: 8px !important;
    box-shadow: rgba(0, 0, 0, 0.15) 1.95px 1.95px 2.6px !important;
}

main div.content a :hover{
    background: #00bfff !important;
    color: white !important;
}

main div.content ul li{
    margin-top: 8px !important;
    margin-bottom: 8px !important;
}


p {
    margin-top: 0.55em;
}

hr {
    width: 100%;
    margin: 1em 0;
    border: 0;
    border-top: 1px solid #ccc;

}

#content{
  background-color: white;
  padding: 20px 40px 80px 80px;
}

h2{  
  background-color: deepskyblue;
    color: white;
  padding-left: 12px;
  margin-top: 40px;
  font-weight: 600;
}

h3{  
  background-color: antiquewhite;
  padding-left: 24px;
  margin-left: 36px; 
  margin-top: 40px; 
}

button.btnSom{
    position: fixed;
    bottom: 40px;
    right: 20px;
}

/* Responsive Dico IA */
@media only screen and (max-width: 600px){
    .main{
        padding: 0;
        margin: 0;
        width: 100%;
        padding-inline: 0;
    }
    #content{
        padding: 8px;
    }

    button.btnSom{
    position: fixed;
    bottom: 70px;
    right: 50px;
}

}


    

</style>  

</head>
<body>

<main>
<!-- <a href="../"><div class="retour">retour</div></a> -->
<a href="#"> <button class="btnSom">sommaire</button></a>

<textarea id="markdown" style="display:none;">
 # Dico des termes est concepts IA

- [Activation function](#activation-function) - fonction d'activation
- [Active parameters](#active-parameters) - param√®tres actifs
- [Apprentissage suppervis√©](#apprentissage-suppervis√©) - supervised learning
- [Apprentissage non supervis√©](#apprentissage-non-supervis√©) - unsupervised learning
- [Autoencoder](#autoencoder) - autoencodeur
- [Autoregressive](#autoregressive) - mod√®le autoregressif
- **Attention crois√©e** - voir [Cross-attention](#cross-attention)
- [Biais](#biais-distorsion) - (distorsion syst√©matique)
- [Biais](#biais-offset) - (offset neurone)
- [Catastrophic forgetting](#catastrophic-forgetting) - oubli catastrophique
- **Clustering** - voir [Apprentissage non supervis√©](#apprentissage-non-supervis√©)
- [Context window](#context-window) - fen√™tre de contexte
- **Couche de normalisation** - voir [Layer Normalization](#layer-normalization)
- [Cross-attention](#cross-attention) - attention crois√©e
- [Cross-entropy](#cross-entropy) - entropie crois√©e
- [Dataset](#dataset)
- **Denoising autoencoder** - voir [Autoencoder](#autoencoder)
- [Decoder-only](#decoder-only)
- **Descente de gradient** - voir [Gradient descend](#gradient-descend)
- [Diffusion model](#diffusion-model) - mod√®le de diffusion
- [DPO](#dpo) - Direct Preference Optimization
- [Encoder-Decoder](#encoder-decoder) - encodeur d√©codeur
- **Entropie crois√©e** - voir [Cross-entropy](#cross-entropy)
- [Embedding vector](#embedding-vector) - vecteur d'embedding
- **Feedforward** - voir [MLP (Multilayer Perceptron)](#mlp)
- **Fen√™tre de contexte** - voir [Context window](#context-window)
- [Fine-tuning](#fine-tuning) - affinage
- **float16**, **float32** - voir [Quantization](#quantization)
- **Fonction de perte** - voir [Loss function](#loss-function)
- **Fonction d'activation** - voir [Activation function](#activation-function)
- **Forward diffusion** - voir [Diffusion model](#diffusion-model)
- [Quantization](#quantization)
- **Quantification** - voir [Quantization](#quantization)
- [Layer Normalization](#layer-normalization) - couche de normalisation
- **LayerNorm** - voir [Layer Normalization](#layer-normalization)
- [LoRA (Low-Rank Adaptation)](#lora)
- [Loss function](#loss-function) - fonction de perte
- [GAN](#gan) - Generative Adversarial Network
- [Gradient descend](#gradient-descend) - descente de gradient
- **Greedy decoding** - voir [Sampling](#sampling)
- **int8** - voir [Quantization](#quantization)
- **KL divergence** - voir [Loss function](#loss-function)
- [Latent space](#latent-space) - espace latent
- [Latent space vs search space](#latent-space-vs-search-space)  - espace latent vs search space
- [Learning-rate](#learning-rate) - taux d'apprantissage
- **Minimiser une fonction de perte** - voir [Gradient descend](#gradient-descend)
- [MLP (Multilayer Perceptron)](#mlp) - perceptron multicouche
- **Mod√®le autoregressif** - voir [Autoregressive](#autoregressive)
- **Mod√®le de diffusion** - voir [Diffusion model](#diffusion-model)
- [Multi-head attention](#multi-head-attention)
- **Oubli catastrophique** - voir [Catastrophic forgetting](#catastrophic-forgetting)
- **Param√®tres actifs** - voir [Active parameters](#active-parameters)
- **Perceptron multicouche** - voir [MLP (Multilayer Perceptron)](#mlp)
- [Prompt-engineering](#prompt-engineering)
- **Pruning** - voir [Active parameters](#active-parameters)
- **ReLU** (Rectified Linear Unit) - voir [Activation function](#activation-function)
- **Replay** (rejeu) - voir [Catastrophic forgetting](#catastrophic-forgetting)
- **Reverse diffusion** - voir [Diffusion model](#diffusion-model)
- [Sampling](#sampling) - √©chantillonnage
- [Sampling vs Transformer](#sampling-vs-transformer) - √©chantillonnage dans architecture Transformer
- **Sigmoid** - voir [Activation function](#activation-function)
- [Softmax](#softmax) - voir aussi [Activation function](#activation-function)
- **Sparse autoencoder** - voir [Autoencoder](#autoencoder)
- **Supervised learning** - voir [Apprentissage suppervis√©](#apprentissage-suppervis√©)
- **Tanh** - voir [Activation function](#activation-function)
- **Tok K sampling** - voir [Sampling](#sampling)
- **Tok P sampling** - voir [Sampling](#sampling)
- **Temperature sampling** - voir [Sampling](#sampling)
- [Tokens](#tokens)
- [Transformer](#transformer)
- **Unsupervised learning** - voir [Apprentissage non supervis√©](#apprentissage-non-supervis√©)
- **Variational autoencoder (VAE)** - voir [Autoencoder](#autoencoder)
- **Vecteur d'embedding** - voir [Embedding vector](#embedding-vector)

<!-- //A FAIRE
A2A (agent to agent)
ADK
Active parametres
Activation space
Agent
Alexnet
ANN (index ivfflat et HNSW)
Architecture s√©quentielle / Architectiture parral√®le
Attention mechanism / Attention layer / shared attention / Attention
BatchNorm
BERT
Bayesian optimization, grid search, random search, ‚Ä¶
Byte-Pair Encoding
BF16
Causal self-attention
Chain of toughts
Couche lin√©aire (projection)
Cold start
Deep Learning
Distribution vraie
Distribution pr√©dite
Dot-product attention
Escape velocity
Entropy
Energy based model (machine de Boltzman)
Expert system
Feedforward
FLAN
FLOPs
Fonction de densit√© de probabilit√©
FP8 / FP16 / 4K presision
G√©n√©ratif / Predictif - discriminant
GPQA
GPT
Hyperpatam√®tres
HUgginface
Information theory
Jax
Keras
Layer
Layer normalization & r√©sidus
LLM
LMM
Logit
LSTM
Matrice de confusion
Mamba
Masked causal self-attention mechanism
MCP - Model Context Protocol
Missalignement / inner missalignement / outer missalignement
MSE (Mean Squared Error)
MoE
Monte-Carlo (Algortihme de Monte-Carlo)
Multimodal
Needle-in-ahaystack
Neural network
NLP
Overfiting
Ollama
Parram√®tres
Parametric model
Perceptron
Pr√©diction
Pre-trainning
Poids du mod√®le
Policy
Post-trainning
Positionnal encoding
PPO
Pytorch
Quantization
Queries / keys /values
Q V cache
R√©duction de dimensionnalit√© 
Repr√©sentation latente de dimension r√©duite
Reward 
Reward hacking
Ring attention
RL
RLHF
RNN
Sampling space
Scaling
Scaling laws
Score L2 / F1 / Pr√©cision
Search space
Seed
Self-attention
SFT
Shared attention
Signal d‚Äôapprentissage
SLM
Softmax
Scalable-Softmax
Stochastic gradient descent (SGD)
Subquadratic model
T5
Tensorflow
Tokeniser
Trainning
Tree of toughts
TTA - Test Time Adaptative
TTC - Test Time Compute
Unigram Language Model
Word2vec
VAE - Autoencoders variationnels
VLM
Wheight and bias 
Zero shot learning
-->

--------------------------------------------------------------------------------------------------------------

## Activation function

Le concept de **fonction d‚Äôactivation** (activation function) est une composante essentielle des r√©seaux de neurones.

### ‚ö° R√¥le

Une fonction d‚Äôactivation **introduit de la non-lin√©arit√© dans le r√©seau.**
Sans elle, m√™me un r√©seau profond ne ferait que des combinaisons lin√©aires des entr√©es, ce qui limiterait fortement sa capacit√© √† mod√©liser des ph√©nom√®nes complexes.

### üîç Fonctionnement

Chaque neurone applique une fonction d‚Äôactivation apr√®s avoir calcul√© une somme pond√©r√©e de ses entr√©es :

![actvation-01](./images/img-dico-ia/activation_fuction.png "activation function")

o√π ùúô est la fonction d‚Äôactivation.

### üß† Fonctions courantes

- **ReLU** (Rectified Linear Unit)

![relu-01](./images/img-dico-ia/relu.png "relu")

Tr√®s utilis√©e car simple, rapide, et efficace pour les couches profondes.

- **Sigmoid**

![sigmoid-01](./images/img-dico-ia/sigmoid.png "sigmoid")

Anciennement populaire, mais peut provoquer des gradients trop petits.

- **Tanh** : similaire √† sigmoid mais centr√©e autour de 0.
- **Softmax** : transforme un vecteur en une distribution de probabilit√© (souvent en sortie de mod√®le de classification ou LLM).

### ‚úÖ En r√©sum√© - Activation function

La fonction d‚Äôactivation donne au r√©seau la capacit√© de mod√©liser des relations complexes et non-lin√©aires.
C‚Äôest elle qui permet aux mod√®les d‚ÄôIA de ¬´ comprendre ¬ª des structures plus riches que des simples r√®gles lin√©aires.

--------------------------------------------------------------------------------------------------------------

## Active parameters

Les **param√®tres actifs** (active parameters) sont une notion importante dans l‚Äôoptimisation et la compression des mod√®les.

### üß† D√©finition - Active parameters

Les **param√®tres actifs** sont les **poids du mod√®le qui sont effectivement utilis√©s ou modifi√©s lors de l‚Äôapprentissage ou de l‚Äôinf√©rence**, par opposition √† des **poids fig√©s**, **inutilis√©s**, ou **masqu√©s** (comme dans les techniques de pruning, sparsity, ou LoRA).

### üîç Pourquoi c‚Äôest important ? - Active parameters

- **Efficacit√©** : r√©duire le nombre de param√®tres actifs permet de limiter les co√ªts m√©moire et calcul.
- **Interpr√©tabilit√©** : identifier les param√®tres qui influencent r√©ellement la sortie du mod√®le aide √† mieux comprendre son comportement.
- **Personnalisation** : dans les techniques comme LoRA ou Adapter layers, seuls certains sous-ensembles de param√®tres sont actifs, ce qui rend les adaptations plus l√©g√®res.

### üß∞ Utilisation - Active parameters

- **Fine-tuning partiel** : on g√®le une partie du mod√®le et on entra√Æne seulement les param√®tres actifs.
- **Pruning** : suppression des poids inactifs (par magnitude ou gradient) pour compresser le mod√®le.
- **Efficient deployment** : ne charger en m√©moire que les blocs de param√®tres r√©ellement n√©cessaires √† une t√¢che ou √† un utilisateur donn√©.

### ‚úÖ En r√©sum√© - Active parameters

Les **param√®tres actifs** sont ceux qui comptent vraiment dans le fonctionnement du mod√®le.
Identifier et exploiter ce sous-ensemble permet d‚Äôam√©liorer la performance, la compr√©hension, et la d√©ployabilit√© des r√©seaux de neurones modernes.

--------------------------------------------------------------------------------------------------------------

## Apprentissage suppervis√©

### üéì D√©finition - Apprentissage suppervis√©

L‚Äô**apprentissage supervis√©** (supervised learning) est une m√©thode o√π le mod√®le apprend √† pr√©dire une sortie √† partir d‚Äôune entr√©e, en utilisant un **jeu de donn√©es annot√©**.
Chaque exemple du dataset contient :

- une entr√©e (ex. : une image, un texte, des caract√©ristiques num√©riques),
- une sortie cible ou √©tiquette (ex. : cat√©gorie, valeur num√©rique, texte attendu).

### üß† Comment √ßa fonctionne ? - Apprentissage suppervis√©

1. On alimente le mod√®le avec des paires entr√©e/sortie.
2. Il fait une pr√©diction.
3. On mesure l‚Äôerreur entre sa pr√©diction et la vraie √©tiquette (via une **fonction de perte**).
4. L‚Äôalgorithme ajuste les **poids du mod√®le** (par **descente de gradient** typiquement).
5. Le processus est r√©p√©t√© jusqu‚Äô√† ce que l‚Äôerreur soit minimale ou satisfaisante.

### üì¶ Exemples d‚Äôapplication - Apprentissage suppervis√©

- Classification d‚Äôimages (chat vs chien),
- D√©tection de spam dans les emails,
- Pr√©diction de prix immobiliers (r√©gression),
- Traduction automatique (avec un jeu de paires phrase-source / phrase-cible).

### ‚úÖ En r√©sum√© - Apprentissage suppervis√©

L‚Äôapprentissage supervis√© est comme un √©l√®ve qui apprend √† partir de corrig√©s : en comparant ses r√©ponses √† celles du professeur, il ajuste sa compr√©hension jusqu‚Äô√† g√©n√©raliser correctement sur de nouveaux cas similaires.

--------------------------------------------------------------------------------------------------------------

## Autoregressive

Le concept d‚Äô**autoregressive** (auto-r√©gressif), est fondamental pour de nombreux mod√®les de g√©n√©ration de texte comme GPT.

### üîÑ Qu‚Äôest-ce qu‚Äôun mod√®le autoregressive ?

Un mod√®le autoregressif pr√©dit chaque √©l√©ment de sortie (token) en se basant uniquement sur les √©l√©ments d√©j√† g√©n√©r√©s ou observ√©s.
Concr√®tement, pour une s√©quence de tokens (x)1, (x)2, ..., (x)n , un mod√®le autoregressif estime la probabilit√© jointe comme :

![autoregressive-01](./images/img-dico-ia/autoregressive_01.png "autoregressive")

Chaque token  (x)i  est g√©n√©r√© s√©quentiellement, en conditionnant sa distribution sur tous les pr√©c√©dents.

### üß† Pourquoi c‚Äôest important ? - Autoregressive

- **Simplicit√© de d√©codage** : on g√©n√®re mot √† mot ou token par token, en utilisant le m√™me r√©seau pour chaque position.
- **Modularit√©** : la pr√©diction se fait √©tape par √©tape, facilitant l‚Äôapplication de diff√©rentes strat√©gies de sampling (greedy, top-k, top-p‚Ä¶).
- **Performance** : cette approche a prouv√© son efficacit√© pour la coh√©rence locale et le maintien du contexte √† court terme.

### üîß Exemples concrets

- **GPT** (Generative Pre-trained Transformer) est un mod√®le decoder-only autoregressif : il lit le contexte pr√©c√©dent puis g√©n√®re le token suivant, et ainsi de suite.
- **WaveNet** (pour l‚Äôaudio) : g√©n√®re chaque √©chantillon sonore en conditionnant sur l‚Äôhistorique audio d√©j√† produit.
- **PixelRNN** / **PixelCNN** en vision : g√©n√®rent chaque pixel en se basant sur les pixels d√©j√† synth√©tis√©s.

### ‚úÖ En r√©sum√© - Autoregressive

Un mod√®le autoregressif produit sa sortie token par token, en s‚Äôappuyant exclusivement sur ce qui a d√©j√† √©t√© g√©n√©r√©. C‚Äôest cette dynamique s√©quentielle qui rend possibles des g√©n√©rations de texte, d‚Äôaudio ou d‚Äôimages coh√©rentes et progressives.

--------------------------------------------------------------------------------------------------------------

## Biais distorsion

### ‚öñÔ∏è Qu‚Äôest-ce qu‚Äôun biais en IA ?

Le **biais** d√©signe une distorsion syst√©matique dans les pr√©dictions ou comportements d‚Äôun mod√®le.
Ce n‚Äôest pas une erreur ponctuelle, mais une tendance persistante √† favoriser certaines sorties ou √† sous-repr√©senter certaines cat√©gories.

Il peut appara√Ætre √† plusieurs niveaux :

- **Dans les donn√©es** (donn√©es d‚Äôentra√Ænement d√©s√©quilibr√©es, st√©r√©otyp√©es, ou mal collect√©es),
- **Dans l‚Äôarchitecture du mod√®le** (choix des fonctions, r√©gularisations, etc.),
- **Dans l‚Äôinterpr√©tation des r√©sultats** (pr√©sentation ou usage biais√© des sorties).

### üß† Types de biais courants

- **Biais de s√©lection** : certaines classes ou groupes sont sur-repr√©sent√©s dans le dataset.
- **Biais de confirmation** : le mod√®le renforce les hypoth√®ses dominantes dans les donn√©es.
- **Biais algorithmique** : li√© au fonctionnement interne du mod√®le, ind√©pendamment des donn√©es.
- **Biais soci√©tal** : le mod√®le reproduit des st√©r√©otypes de genre, d‚Äôorigine, ou d‚Äô√¢ge pr√©sents dans les donn√©es.

### üõ†Ô∏è Pourquoi c‚Äôest un probl√®me ?

- Cela peut mener √† des discriminations (ex. : refus de cr√©dit pour certains profils),
- Fausser les pr√©dictions dans des domaines sensibles (sant√©, justice, √©ducation),
- Nuire √† la fiabilit√© et l‚Äô√©quit√© du mod√®le.

### ‚úÖ En r√©sum√© (biais - distortion)

Le biais en IA n‚Äôest pas une simple erreur technique : c‚Äôest une d√©viation structurelle qui peut avoir des cons√©quences majeures.
Le d√©tecter, le mesurer et le r√©duire est une √©tape cruciale dans le d√©ploiement de mod√®les √©thiques et robustes.

--------------------------------------------------------------------------------------------------------------

## Biais offset

### 1. Le biais comme param√®tre d‚Äôun neurone (offset)

Dans un r√©seau de neurones, chaque neurone calcule une somme pond√©r√©e de ses entr√©es, puis lui ajoute un biais avant d‚Äôappliquer la fonction d‚Äôactivation.
Formellement, pour un neurone :

z = (w1)(x1) + (w2)(x2) + ... + (wn)(xn) + (b)

- w(i) sont les **poids** (weights),
- b est le **biais** (bias).

### üîç R√¥le du biais

- Il permet au neurone de se d√©caler : sans biais, la sortie d‚Äôun neurone serait forc√©ment nulle quand toutes les entr√©es sont √† z√©ro.
- Le biais agit comme une ordonn√©e √† l‚Äôorigine (intercept) dans une r√©gression lin√©aire, autorisant le mod√®le √† mieux s‚Äôajuster aux donn√©es.

### üõ†Ô∏è Apprentissage

- Le biais est un **param√®tre appris** durant la **descente de gradient**, exactement comme les **poids**.
- Il a son propre gradient *‚àÇùêø/‚àÇùëè* et est mis √† jour √† chaque it√©ration.

### ‚úÖ En r√©sum√© (biais - offset)

- Biais (bias) en tant que param√®tre : un offset appris *b* ajout√© √† la somme pond√©r√©e dans chaque neurone, qui permet de d√©caler la fonction d‚Äôactivation.

--------------------------------------------------------------------------------------------------------------

## Apprentissage non supervis√©

L‚Äôapprentissage non supervis√© (unsupervised learning), une m√©thode cl√© du machine learning.

### üé≤ D√©finition

L‚Äôapprentissage non supervis√© consiste √† faire apprendre un mod√®le uniquement √† partir de donn√©es **non √©tiquet√©es**, c‚Äôest-√†-dire sans fournir de ‚Äúbonne r√©ponse‚Äù √† chaque exemple.
Le **but est de d√©couvrir des structures, des motifs ou des groupements inh√©rents aux donn√©es**.

### üß© Principales techniques

- **Clustering** : regrouper des objets similaires
  - *K-means* : partitionne les donn√©es en K clusters en minimisant la variance intra-groupe.
  - *DBSCAN* : identifie des clusters de densit√© sans fixer √† l‚Äôavance le nombre de groupes.
- **R√©duction de dimensionnalit√©** : projeter les donn√©es dans un espace plus petit tout en pr√©servant leur structure
  - *PCA* (Analyse en Composantes Principales)
  - *t-SNE*, *UMAP* pour la visualisation
- **Mod√®les g√©n√©ratifs** : apprendre la distribution des donn√©es pour g√©n√©rer de nouveaux exemples
  - *Autoencodeurs*
  - *Variational Autoencoders* (VAE)
  - *GANs* (Generative Adversarial Networks)

### üîç Pourquoi l‚Äôutiliser ?

- **Exploration des donn√©es** : comprendre la structure et les sous-groupes avant d‚Äôentra√Æner des mod√®les supervis√©s.
- **Pr√©traitement** : r√©duire la dimensionnalit√© pour acc√©l√©rer l‚Äôapprentissage ou am√©liorer la visualisation.
- **D√©tection d‚Äôanomalies** : rep√©rer des points de donn√©es inhabituels qui ne s‚Äôint√®grent pas bien dans les clusters principaux.

### ‚úÖ En r√©sum√© - Apprentissage non supervis√©

L‚Äôapprentissage non supervis√© permet au mod√®le de d√©couvrir par lui-m√™me des motifs dans des donn√©es brutes.
C‚Äôest une √©tape pr√©cieuse pour explorer, structurer ou g√©n√©rer des donn√©es sans n√©cessiter d‚Äô√©tiquetage manuel.

--------------------------------------------------------------------------------------------------------------

## Context window

### üß† Qu‚Äôest-ce que le *context window* ?

C‚Äôest la **quantit√© maximale de texte** (en tokens) qu‚Äôun mod√®le de langage peut lire et traiter en une seule fois.
Les LLM ne ¬´ comprennent ¬ª pas un texte dans son int√©gralit√© : ils le d√©coupent en morceaux de taille limit√©e, appel√©s *fen√™tres de contexte*.

### üìè Taille typique

- GPT-3 : jusqu‚Äô√† 4 096 tokens.
- GPT-4 : selon la version, de 8 192 √† 128 000 tokens.
- Claude 3 : jusqu‚Äô√† 200 000 tokens.
- 22/04/2025 :
  - Llama 4 scout : 10M
  - Gemnini 2.5 : 1M
  - GTP 4.1 : 1M

Un *token* est une unit√© textuelle, souvent plus courte qu‚Äôun mot (ex : ¬´ internationalisation ¬ª = 3 tokens).

### ‚öôÔ∏è Pourquoi c‚Äôest important ?

La fen√™tre de contexte d√©termine :

- la capacit√© du mod√®le √† comprendre des documents longs,
- la coh√©rence des r√©ponses,
- la qualit√© du raisonnement si le mod√®le doit se souvenir de nombreuses informations.

### üß© Limite & strat√©gies

Quand un texte d√©passe cette taille, il faut **tronquer**, **r√©sumer**, ou utiliser des techniques comme le **retrieval-augmented generation (RAG)** pour garder un acc√®s contextuel large.

### ‚úÖ En r√©sum√©

Le *context window* est la m√©moire imm√©diate du LLM : plus elle est grande, plus le mod√®le peut manipuler de texte √† la fois, ce qui augmente sa puissance dans les t√¢ches complexes.

--------------------------------------------------------------------------------------------------------------

## Softmax

Le concept de fonction softmax, est tr√®s utilis√© dans les mod√®les de classification et dans les LLMs.

### üìê Qu‚Äôest-ce que la softmax ?

La fonction softmax transforme un vecteur de scores (appel√©s logits) en une distribution de probabilit√©.
Chaque valeur produite par la softmax est comprise entre 0 et 1, et la somme de toutes les valeurs est √©gale √† 1.

Formellement, pour un vecteur (Z) = (Z1, Z2, ‚Ä¶, Zn), la sortie de la softmax est :

![softmax](./images/img-dico-ia/softmax.png "softmax")

### üß† Pourquoi l‚Äôutiliser ?

- Elle permet de convertir les **logits bruts** d‚Äôun r√©seau (souvent tr√®s dispers√©s) en probabilit√©s interpr√©tables.
- Elle est utilis√©e en **fin de r√©seau** pour des t√¢ches de classification (ex. : pr√©dire la probabilit√© d‚Äôappartenance √† une classe).
- Dans les **LLM**, elle sert √† transformer les sorties du mod√®le en d**istribution de probabilit√© sur le vocabulaire** pour permettre le sampling du prochain token.

### üéØ Propri√©t√©s

- Accentue les diff√©rences : **les plus grands logits obtiennent une probabilit√© plus √©lev√©e, et les petits logits sont √©cras√©s**.
- Sensible √† l‚Äô√©chelle : **les logits tr√®s grands peuvent dominer l‚Äôensemble de la distribution (d‚Äôo√π l‚Äôusage fr√©quent d‚Äôun facteur de temp√©rature pour moduler cet effet)**.

### ‚úÖ En r√©sum√© - Softmax

La *softmax* **transforme un vecteur num√©rique en distribution probabiliste**, permettant de choisir ou d‚Äô√©chantillonner une sortie dans des t√¢ches de g√©n√©ration ou de classification.
C‚Äôest l‚Äô√©tape finale incontournable de nombreux r√©seaux de neurones.

--------------------------------------------------------------------------------------------------------------

## Autoencoder

L'**autoencodeur** (autoencoder), est une architecture couramment utilis√©e dans l‚Äôapprentissage non supervis√© et les mod√®les g√©n√©ratifs.

### üîÑ D√©finition : autoencoder

Un **autoencodeur** est un r√©seau de neurones con√ßu pour reproduire son entr√©e √† la sortie.
Il est constitu√© de deux parties :

1. **Encodeur** : compresse les donn√©es d‚Äôentr√©e en une repr√©sentation latente de dimension r√©duite.
2. **D√©codeur** : reconstruit l‚Äôentr√©e originale √† partir de cette repr√©sentation comprim√©e.

### üß† Pourquoi utiliser un autoencodeur ?

L‚Äôobjectif n‚Äôest pas seulement de recopier les donn√©es, mais de forcer le mod√®le √† apprendre les caract√©ristiques essentielles des donn√©es pour les repr√©senter de mani√®re compacte.

Cela permet de :

- d√©tecter des motifs cach√©s,
- r√©duire la dimension des donn√©es,
- g√©n√©rer de nouvelles donn√©es proches de l‚Äôoriginal,
- d√©tecter des anomalies (une reconstruction de mauvaise qualit√© peut signaler un cas atypique).

### üì¶ Variantes connues

- **Denoising autoencoder** : apprend √† reconstruire une entr√©e propre √† partir d‚Äôune version bruit√©e.
- **Variational autoencoder (VAE)** : apprend une distribution probabiliste sur l‚Äôespace latent, utilis√© pour la g√©n√©ration.
- **Sparse autoencoder** : contraint la repr√©sentation latente √† n‚Äôactiver qu‚Äôun petit nombre de neurones.

### ‚úÖ En r√©sum√© - Autoencoder

Les autoencodeurs sont des outils puissants pour apprendre des repr√©sentations compactes des donn√©es.
Ils sont largement utilis√©s en compression, d√©tection d‚Äôanomalies, et comme fondation pour des mod√®les g√©n√©ratifs comme les VAE.

--------------------------------------------------------------------------------------------------------------

## Cross-attention

Le m√©canisme de **cross-attention** (attention crois√©e), essentiel dans les architectures encoder-decoder comme T5 ou BART.

### üîÑ Qu‚Äôest-ce que la cross-attention ?

La **cross-attention** est une variante de l‚Äôattention qui relie deux s√©quences diff√©rentes :

- **les requ√™tes** (*queries*) proviennent d‚Äôune s√©quence (souvent la sortie partielle du d√©codeur),
- les **cl√©s** (*keys*) et **valeurs** (*values*) proviennent d‚Äôune autre s√©quence (g√©n√©ralement la repr√©sentation produite par l‚Äôencodeur).
- Contrairement √† la *self-attention*, qui compare chaque position d‚Äôun m√™me √©l√©ment √† lui-m√™me, la cross-attention permet au d√©codeur de ¬´ regarder ¬ª directement les informations encod√©es de l‚Äôentr√©e.

### ‚öôÔ∏è Comment √ßa fonctionne ? - Cross-attention

1. **Requ√™tes Q** : issues de l‚Äô√©tat actuel du d√©codeur (√† la couche i).
2. **Cl√©s K** et **Valeurs V**: extraites de la sortie de l‚Äôencodeur (m√™me couche i).
3. On calcule les scores d‚Äôattention :
![cross-attention](./images/img-dico-ia/cross-attention.png "cross-attention")
4. On applique un softmax sur ces scores pour obtenir des poids.
5. On pond√®re V par ces poids, ce qui donne une contribution contextuelle suppl√©mentaire √† la g√©n√©ration du texte.

### ü§î Pourquoi c‚Äôest important ? - Cross-attention

- **Alignement** : le d√©codeur peut s‚Äôappuyer directement sur les informations cl√©s de l‚Äôentr√©e, ce qui am√©liore la qualit√© des traductions, r√©sum√©s ou toute t√¢che de transformation de s√©quence √† s√©quence.
- **Efficacit√©** : en s√©parant self-attention (qui g√®re la coh√©rence interne de la s√©quence en cours de g√©n√©ration) et cross-attention (qui introduit l‚Äôinformation externe), on obtient un mod√®le plus modulable et performant.

### ‚úÖ En r√©sum√© - Cross-attention

La **cross-attention** est le lien entre encodeur et d√©codeur : elle permet √† chaque √©tape de g√©n√©ration de puiser dans la repr√©sentation de l‚Äôentr√©e pour produire une sortie plus pr√©cise et contextuellement pertinente.

*(suite) pour la formule de calcul du score d'o√π viennent l'exposant T et la racine d ?*

Excellente question ! La formule standard de l‚Äôattention (dot-product attention) est :

![dot-product_attention](./images/img-dico-ia/dot-product_attention.png "dot-product_attention")

Voici d‚Äôo√π viennent exactement le **T** (transpose) et la **racine** :

![dot-product_attention_Transpose](./images/img-dico-ia/dot-product_attention_Transpose.png "dot-product_attention_Transpose")
![dot-product_attention_Factor](./images/img-dico-ia/dot-product_attention_Factor.png "dot-product_attention_Factor")

En r√©sum√© :

![dot-product_attention_resume](./images/img-dico-ia/dot-product_attention_resume.png "dot-product_attention_resume")

--------------------------------------------------------------------------------------------------------------

## Catastrophic forgetting

### üß† Qu‚Äôest-ce que le catastrophic forgetting ?

C‚Äôest le ph√©nom√®ne par lequel un mod√®le de machine learning, lorsqu‚Äôil est entra√Æn√© s√©quentiellement sur plusieurs t√¢ches, perd brutalement sa capacit√© √† bien r√©aliser les premi√®res t√¢ches d√®s qu‚Äôil apprend les suivantes.
Autrement dit, il ¬´ oublie ¬ª ce qu‚Äôil avait pr√©c√©demment appris.

### ‚öôÔ∏è Pourquoi √ßa se produit ?

Lorsqu‚Äôon ajuste les poids du r√©seau sur une nouvelle t√¢che, la descente de gradient modifie la m√™me m√©moire (les m√™mes param√®tres) qui stockait l‚Äôinformation des t√¢ches ant√©rieures.

Sans m√©canisme prot√©geant ces anciens acquis, les nouveaux ajustements les effacent.

### üõ†Ô∏è Strat√©gies pour limiter l‚Äôoubli

1. **Replay (rejeu)** : conserver un petit buffer de donn√©es des anciennes t√¢ches et les r√©utiliser en entra√Ænement mixte.
2. **Regularization-based methods** : ajouter une p√©nalit√© dans la fonction de perte pour conserver les poids importants pour les t√¢ches pr√©c√©dentes (ex. : Elastic Weight Consolidation).
3. **Architectural approaches** : allouer des sous-r√©seaux ou des modules distincts pour chaque t√¢che, afin de ne pas mutualiser tous les param√®tres.
4. **Progressive networks** : empiler de nouveaux r√©seaux tout en gardant les anciens fig√©s (sans les modifier).

### ‚úÖ En r√©sum√© - Catastrophic forgetting

Le catastrophic forgetting appara√Æt quand un mod√®le form√© s√©quentiellement sur plusieurs jeux de donn√©es n‚Äôarrive pas √† conserver ses performances pass√©es.
G√©rer cet oubli est essentiel pour d√©velopper des IA capables d‚Äôapprentissage continu et d‚Äôadaptation progressive sans devoir tout r√©entra√Æner de z√©ro √† chaque nouvelle t√¢che.

--------------------------------------------------------------------------------------------------------------

## Cross-entropy

La **cross-entropy** est une **fonction de perte** largement utilis√©e dans les mod√®les de classification et les LLM.

### üìâ Qu‚Äôest-ce que la cross-entropy ?

La **cross-entropy** (entropie crois√©e) mesure la **distance** entre deux distributions de probabilit√© :

1. La **distribution vraie** (souvent repr√©sent√©e comme un vecteur one-hot indiquant la bonne classe),
2. La **distribution pr√©dite** (sortie du mod√®le, g√©n√©ralement apr√®s une softmax).

Formule :

![cross-entropy](./images/img-dico-ia/cross-entropy.png "cross-entropy")

o√π (y)i est la probabilit√© vraie (1 pour la classe correcte, 0 sinon) et (p)i la probabilit√© pr√©dite.

### üß† Pourquoi l‚Äôutiliser ? - Cross-entropy

- **Convient aux probl√®mes de classification multi-classes**, notamment avec **softmax** en sortie.
- Elle **p√©nalise fortement** les **erreurs de pr√©diction confiantes et incorrectes**.
- Elle a un **bon comportement math√©matique** pour la descente de gradient, avec une courbe de perte bien form√©e.

### ‚úÖ En r√©sum√© - Cross-entropy

**La cross-entropy quantifie √† quel point les pr√©dictions du mod√®le diff√®rent de la v√©rit√©.**
C‚Äôest la **fonction de perte de r√©f√©rence pour les mod√®les de classification**, en particulier ceux bas√©s sur des distributions de probabilit√© comme les LLM.

--------------------------------------------------------------------------------------------------------------

## Dataset

### üìö Qu‚Äôest-ce qu‚Äôun dataset ?

Un dataset est simplement un ensemble organis√© de donn√©es utilis√© pour entra√Æner, valider ou tester un mod√®le d‚ÄôIA.
Il peut contenir :

- des textes (en NLP),
- des images (en vision par ordinateur),
- des sons,
- ou des structures complexes (comme des graphes ou des s√©ries temporelles).

Chaque √©l√©ment d'un dataset correspond g√©n√©ralement √† une entr√©e (input) et parfois √† une √©tiquette (label), dans le cas de l‚Äôapprentissage supervis√©.

### üî• Caract√©ristiques importantes d‚Äôun bon dataset

- **Qualit√©** : les donn√©es doivent √™tre pertinentes, bien annot√©es et repr√©sentatives du probl√®me √† r√©soudre.
- **Quantit√©** : un volume suffisant est souvent n√©cessaire pour entra√Æner un mod√®le performant.
- **Diversit√©** : il faut couvrir un maximum de cas possibles pour √©viter le surapprentissage (overfitting).

### üèóÔ∏è Quelques exemples c√©l√®bres

- **ImageNet** : pour la reconnaissance d‚Äôimages.
- **COCO** : pour la d√©tection d‚Äôobjets.
- **Common Crawl** : pour l'entra√Ænement de grands mod√®les de langage (√©norme collection de pages web).
- **CIFAR-10** : petit dataset d‚Äôimages pour d√©buter en deep learning.

‚úÖ En r√©sum√©

Un dataset est la "nourriture" de l‚ÄôIA.
Sans donn√©es fiables et bien structur√©es, m√™me le mod√®le le plus avanc√© ne pourra rien apprendre correctement.

--------------------------------------------------------------------------------------------------------------

## Decoder-only

Q: Comment un mod√®le **decoder-only** comme GPT peut malgr√© tout ‚Äúcomprendre‚Äù le texte qu‚Äôon lui donne (voir encoder/decoder).

### üîç Le r√¥le de l‚Äô**embedding** et du **positional encoding**

1. **Embedding** : chaque mot (ou token) est d‚Äôabord converti en vecteur num√©rique.  
2. **Positional encoding** : on ajoute √† ces vecteurs une information sur leur position dans la s√©quence.  

Ces deux √©tapes donnent au mod√®le une repr√©sentation initiale du texte, pr√™te √† √™tre trait√©e par les couches suivantes.

### ü§ù L‚Äô**auto-attention causale** (causal self-attention)

‚Äì Dans un transformer decoder-only, chaque couche contient un m√©canisme de **self-attention** masqu√©e (causale).  
‚Äì **Masqu√©e** signifie que, pour chaque position *i*, le mod√®le ne peut ‚Äúregarder‚Äù que les positions ‚â§ *i*.  
‚Äì En calculant des poids d‚Äôattention, il apprend √† relier chaque token √† l‚Äôensemble du contexte pr√©c√©dent, capturant ainsi le sens global de la s√©quence.

### üèóÔ∏è Pas d‚Äôencodeur s√©par√©, mais un flux unifi√©

Contrairement √† une architecture **encoder-decoder** (o√π l‚Äôencodeur produit d‚Äôabord un √©tat interm√©diaire qu‚Äôun d√©codeur lit ensuite), le decoder-only :

1. **Incr√©mente** l‚Äô√©tat interne au fur et √† mesure qu‚Äôil parcourt la s√©quence : chaque couche affine la repr√©sentation du contexte.  
2. **Consolide** dans ses couches profondes une compr√©hension de plus en plus riche : on pourrait voir ces couches comme un encodeur interne ‚Äúimbriqu√©‚Äù dans le d√©codeur.  
3. **Produit** enfin, √† la sortie, les probabilit√©s du prochain token, en utilisant cette repr√©sentation contextualis√©e.

### ‚úÖ En r√©sum√© - D√©coder-only

M√™me sans un bloc encodeur d√©di√©, GPT ‚Äúcomprend‚Äù gr√¢ce √† ses couches successives de self-attention causale et de feed-forward : ces op√©rations it√©ratives construisent une repr√©sentation du texte √©tape par √©tape, permettant √† la fois d‚Äôanalyser le contexte et de g√©n√©rer la suite la plus coh√©rente.

--------------------------------------------------------------------------------------------------------------

## Diffusion model

Le  mod√®le de diffusion (diffusion model), est un type de g√©n√©rateur de donn√©es populaire pour images (en √©tude pour la g√©n√©ration de code - 2025).

### üå´Ô∏è Qu‚Äôest-ce qu‚Äôun mod√®le de diffusion ?

Un mod√®le de diffusion est un r√©seau g√©n√©ratif qui apprend √† transformer du bruit al√©atoire en donn√©es structur√©es (images, audio, etc.) via un processus en deux phases :

- **Diffusion avant** (forward diffusion) : on **ajoute progressivement du bruit gaussien √† des donn√©es r√©elles**, jusqu‚Äô√† obtenir presque du bruit pur.
- **Diffusion inverse** (reverse diffusion) : on **entra√Æne un mod√®le √† estimer comment retirer ce bruit**, √©tape par √©tape, pour r√©cup√©rer les donn√©es originales.

### üîÑ M√©canisme de g√©n√©ration

1. On part d‚Äôun vecteur latent initial (ùë•)ùëá ‚àº ùëÅ(0,ùêº) (du bruit).
2. Le mod√®le pr√©dit, √† chaque pas ùë°, comment obtenir une version l√©g√®rement moins bruit√©e (ùë•)ùë°‚àí1 √† partir de (ùë•)ùë°
3. Apr√®s ùëá it√©rations de d√©bruitage, on aboutit √† un √©chantillon ùë•0 qui **ressemble** √† la vraie donn√©e.

### üß† Pourquoi c‚Äôest performant ?

- **Qualit√© d‚Äôimage** : produit souvent des images plus nettes et d√©taill√©es que les GANs, sans les probl√®mes de mode collapse.
- **Contr√¥le** : permet de guider la g√©n√©ration (via classifier guidance ou classifier-free guidance) pour orienter le style, le contenu ou la coh√©rence s√©mantique.
- **Stabilit√© d‚Äôentra√Ænement** : l‚Äôoptimisation est plus simple car on minimise une somme de pertes de reconstruction de bruit √† chaque √©tape.

### üõ†Ô∏è Applications

- Synth√®se d‚Äôimages photor√©alistes (Stable Diffusion, DALL¬∑E 2).
- G√©n√©ration audio (Voice Diffusion).
- Data augmentation pour la vision ou le traitement du signal.
- Inpainting et restauration d‚Äôimages.
- G√©n√©ration utlra rapise de code (√† l'√©tude - 2025).

### ‚úÖ En r√©sum√© - Mod√®le de diffusion

Les mod√®les de diffusion g√©n√®rent des donn√©es en apprenant √† inverser un processus de ‚Äúbruitage‚Äù progressif.
Leur capacit√© √† produire des √©chantillons de haute qualit√© et leur flexibilit√© en font aujourd‚Äôhui l‚Äôune des familles de mod√®les g√©n√©ratifs les plus puissantes.

--------------------------------------------------------------------------------------------------------------

## DPO

### üîç Qu‚Äôest-ce que **DPO** (Direct Preference Optimization) ?

C‚Äôest une **m√©thode d‚Äôentra√Ænement bas√©e sur des pr√©f√©rences**.
Plut√¥t que d‚Äôapprendre √† produire *la* bonne r√©ponse, on apprend √† **pr√©f√©rer une r√©ponse √† une autre**.

### üß† Le principe

Imagine qu‚Äôon donne au mod√®le deux r√©ponses possibles √† une question :

- R√©ponse A : correcte, claire, sympa
- R√©ponse B : un peu floue ou moins bonne

On dit au mod√®le :
> "Entre les deux, **pr√©f√®re A**"

Et le mod√®le apprend √† **augmenter la probabilit√© de A** et **diminuer celle de B**.

### üìà En pratique, comment √ßa marche ?

Le mod√®le est entra√Æn√© √† **maximiser une fonction de pr√©f√©rence** sans passer par des r√©compenses RL complexes.
Voici les √©tapes :

1. On g√©n√®re des paires de r√©ponses : **(r√©ponse gagnante, r√©ponse perdante)**.
2. On mesure la **probabilit√© que le mod√®le accorde √† chacune**.
3. On applique une **fonction de perte (loss)** qui pousse le mod√®le √† donner plus de score √† la r√©ponse gagnante.

C‚Äôest **beaucoup plus simple** que les techniques de RL classiques comme PPO (Proximal Policy Optimization), **plus stable**, et **moins co√ªteux**.

### ‚öôÔ∏è Pourquoi "Direct" ?

Parce qu‚Äôon **n‚Äôa pas besoin d‚Äôun reward model externe** (comme dans RLHF) ni d‚Äôune boucle de sampling complexe :

- pas de r√©compense √† mod√©liser,
- pas de policy √† optimiser de mani√®re indirecte,
- on travaille **directement avec les pr√©f√©rences**.

### üß© Dans le cas de LLaMA 4

Ils l‚Äôont utilis√© en **"lightweight"** (l√©ger), √† la **fin** de leur pipeline, pour :

- **corriger les cas limites**,
- **affiner le style des r√©ponses**,
- **am√©liorer la convivialit√©, la clart√©, ou la pr√©cision**,
sans trop figer la cr√©ativit√© ou le raisonnement.

### üß™ En r√©sum√©

| Terme | Explication simple |
|------|--------------------|
| DPO | M√©thode pour dire au mod√®le : "pr√©f√©rez cette r√©ponse plut√¥t qu‚Äôune autre" |
| Avantages | Simple, stable, pas besoin d‚Äôun reward model |
| Utilisation | Derni√®re √©tape pour affiner le comportement du mod√®le |

--------------------------------------------------------------------------------------------------------------

## Encoder Decoder

### üß© Encoder

L‚Äô**encoder** (encodeur) prend une entr√©e (comme une phrase) et la transforme en une **repr√©sentation interne** appel√©e *embedding* ou *vecteur de contexte*.
Il extrait les caract√©ristiques importantes du contenu pour que le mod√®le puisse en comprendre le sens, sans encore produire une sortie textuelle.

### üì¶ Decoder

Le **decoder** (d√©codeur) prend cette repr√©sentation interne produite par l‚Äôencoder et g√©n√®re une **sortie**, souvent mot par mot.
Par exemple, dans la traduction automatique, le decoder produit une phrase dans une autre langue en se basant sur l‚Äôinformation encod√©e.

### ‚öôÔ∏è Utilisation dans les transformers

- **Encoder-only** (ex: BERT) : utile pour l‚Äôanalyse de texte (classification, Q&A).
- **Decoder-only** (ex: GPT) : utile pour la g√©n√©ration de texte.
- **Encoder-Decoder** (ex: T5, BART) : utile pour des t√¢ches comme la traduction ou le r√©sum√©.

### ‚úÖ En r√©sum√© - Encoder/Decoder

- L‚Äô**encoder** comprend le texte.
- Le **decoder** g√©n√®re une r√©ponse.

Ce duo est essentiel pour structurer les flux de traitement dans de nombreux mod√®les d‚ÄôIA modernes.

--------------------------------------------------------------------------------------------------------------

## Embedding vector

Un concept important en Machine Learning et dans l‚Äôutilisation des mod√®les de langage comme les LLM (Large Language Models) est celui d‚Äô**embedding vector** ou **repr√©sentation vectorielle**.

### üîç Embedding vector : qu‚Äôest-ce que c‚Äôest ?

Un embedding vector est une mani√®re de repr√©senter des donn√©es complexes (comme des mots, des phrases ou m√™me des images) sous la forme de vecteurs dans un espace num√©rique multidimensionnel.
L‚Äôid√©e est de transformer des informations qualitatives en une repr√©sentation num√©rique qui capture les relations entre ces informations.

### üß† Pourquoi utilise-t-on des embeddings ?

Dans des domaines comme le traitement du langage naturel (NLP), les mots ou les phrases ne sont pas directement compr√©hensibles pour un mod√®le math√©matique.
On a donc besoin d‚Äôune m√©thode pour convertir ces donn√©es textuelles en nombres.
Mais ce n‚Äôest pas juste une conversion arbitraire : les embeddings sont con√ßus de mani√®re √† ce que des √©l√©ments similaires (par exemple, des mots qui apparaissent souvent ensemble ou qui ont un sens proche) soient proches dans l‚Äôespace vectoriel.
Cela permet au mod√®le d‚ÄôIA d‚Äôidentifier des patterns et des relations, facilitant ainsi l‚Äôapprentissage et la prise de d√©cisions.

### üì¶ Un exemple concret

Imaginons qu‚Äôon travaille sur des embeddings de mots :

- Le mot ¬´ roi ¬ª pourrait √™tre repr√©sent√© par le vecteur [0.2, 0.8, 0.1, ‚Ä¶].
- Le mot ¬´ reine ¬ª par [0.2, 0.75, 0.12, ‚Ä¶].
- Et le mot ¬´ voiture ¬ª par [0.5, 0.1, 0.9, ‚Ä¶].

Quand on regarde cet espace vectoriel, on peut observer que ¬´ roi ¬ª et ¬´ reine ¬ª sont proches l‚Äôun de l‚Äôautre, car ils partagent une relation conceptuelle, alors que ¬´ voiture ¬ª est beaucoup plus √©loign√©.
Cela donne au mod√®le une capacit√© √† comprendre les similarit√©s s√©mantiques, m√™me si ces relations n‚Äô√©taient pas explicitement cod√©es.

### ‚öôÔ∏è Applications

Les embeddings sont utilis√©s dans :

- Les moteurs de recherche (par exemple, pour trouver des documents similaires √† une requ√™te).
- La traduction automatique (pour relier des concepts similaires entre langues).
- Les recommandations (identifier des utilisateurs ou produits similaires).
- Et bien s√ªr, dans les LLM pour comprendre et g√©n√©rer du langage de mani√®re plus coh√©rente et pertinente.

En r√©sum√©, l‚Äôembedding vector est un des piliers fondamentaux permettant aux syst√®mes d‚ÄôIA de repr√©senter et de manipuler des informations complexes de fa√ßon num√©rique, tout en capturant les relations et les similarit√©s qui se trouvent dans les donn√©es.

--------------------------------------------------------------------------------------------------------------

## Fine-tuning

### üß© Qu‚Äôest-ce que le fine-tuning ?

Le fine-tuning (ou ajustement fin) consiste √† prendre un mod√®le pr√©-entra√Æn√© (comme GPT ou BERT) et √† le r√©entra√Æner l√©g√®rement sur un jeu de donn√©es sp√©cifique pour l‚Äôadapter √† une t√¢che particuli√®re.
Cela permet de conserver les capacit√©s g√©n√©rales du mod√®le tout en le sp√©cialisant.

### üîç Embdding vector: Pourquoi l‚Äôutiliser ?

Plut√¥t que d‚Äôentra√Æner un mod√®le depuis z√©ro (ce qui est tr√®s co√ªteux en donn√©es et en puissance), on peut affiner un mod√®le existant avec des exemples sp√©cialis√©s.
Par exemple :

- Adapter un LLM g√©n√©raliste √† du vocabulaire juridique ou m√©dical.
- Entra√Æner un chatbot pour une entreprise avec ses propres donn√©es.

### üß† fine-tuning : Comment √ßa fonctionne ?

On fournit au mod√®le des **paires entr√©e ‚Üí sortie sp√©cifiques √† la t√¢che**, et on l‚Äôentra√Æne sur ces exemples avec un taux d‚Äôapprentissage faible pour ne pas ¬´ oublier ¬ª ce qu‚Äôil a d√©j√† appris.
Cela permet d‚Äôam√©liorer ses performances sur un domaine pr√©cis.

Le fine-tuning est un outil puissant pour cr√©er des IA adapt√©es √† des usages concrets, tout en √©conomisant temps et ressources.

--------------------------------------------------------------------------------------------------------------

## Quantization

La **quantization** (ou quantification), une technique utilis√©e pour rendre les mod√®les de deep learning plus efficaces.

### ‚öôÔ∏è Qu‚Äôest-ce que la quantization ?

La quantization consiste √† **convertir les poids et/ou les activations d‚Äôun mod√®le de haute pr√©cision** (par exemple en 32 bits flottants, float32) **vers une pr√©cision plus faible** (comme int8 ou float16), tout en maintenant au mieux ses performances.

### üéØ Pourquoi le faire ?

- **R√©duction de la taille m√©moire** : un mod√®le quantifi√© occupe beaucoup moins d‚Äôespace.
- **Acc√©l√©ration des inf√©rences** : les op√©rations sur des entiers ou des flottants r√©duits sont plus rapides sur du mat√©riel optimis√© (GPU, TPU, CPU ARM).
- **Efficacit√© √©nerg√©tique** : utile pour le d√©ploiement sur des appareils embarqu√©s ou mobiles.

### üß† Types de quantization

- **Post-training quantization** : on quantifie un mod√®le d√©j√† entra√Æn√©.
- **Quantization-aware training (QAT)** : le mod√®le est entra√Æn√© en simulant la quantification, ce qui permet une meilleure pr√©cision apr√®s compression.
- **Dynamic / Static quantization** : selon si l‚Äô√©chelle des valeurs est ajust√©e √† la vol√©e ou pr√©-calcul√©e.

### üìâ Inconv√©nients

- Peut d√©grader les performances si mal calibr√©e.
- Certaines architectures ou op√©rations sont plus sensibles √† la perte de pr√©cision.

### ‚úÖ En r√©sum√© - Quantization

La quantization est une technique d‚Äôoptimisation mat√©rielle qui permet de r√©duire les co√ªts de calcul et de m√©moire d‚Äôun mod√®le, souvent avec une tr√®s l√©g√®re perte de pr√©cision.
Elle est essentielle pour rendre les LLMs plus accessibles en production ou sur des dispositifs contraints.

--------------------------------------------------------------------------------------------------------------

## Layer Normalization

Les Layer Normalization (couche de normalisations), sont tr√®s utilis√©s dans les architectures de type transformer.

### ‚öôÔ∏è Qu‚Äôest-ce que la Layer Normalization ?

La **LayerNorm** est une op√©ration qui normalise les activations d‚Äôune m√™me couche pour chaque √©chantillon, en recentrant et redimensionnant les valeurs afin qu‚Äôelles aient **moyenne nulle** et **variance unitaire**.

![layer-normalization](./images/img-dico-ia/normalization_layer.png "layer-normalization")

### üß† Pourquoi l‚Äôutiliser ? - Layer Normalization

- **Stabilit√© de l‚Äôentra√Ænement** : r√©duit la covariance shift interne, aidant les gradients √† rester bien conditionn√©s.
- **Ind√©pendance du batch size** : contrairement √† la **BatchNorm**, **LayerNorm** ne d√©pend que de chaque √©chantillon individuellement, ce qui est crucial pour les mod√®les s√©quentiels ou en inference d√©coupl√©e.
- **Rapidit√© de convergence** : facilite et acc√©l√®re l‚Äôapprentissage, en particulier dans les r√©seaux profonds comme les transformers.

### üîó Interaction avec les connexions r√©siduelles

Dans les transformers, on place souvent la LayerNorm **avant** et/ou **apr√®s** chaque **bloc d‚Äôattention** ou de **feed-forward**, √† l‚Äôint√©rieur de la connexion r√©siduelle (residual connection) :

Output = LayerNorm(x + Sublayer(x))

Cela permet de stabiliser le passage d‚Äôinformations tout en conservant l‚Äôidentit√© x et en ajoutant la transformation Sublayer(x).

### ‚úÖ En r√©sum√© - Layer Normalization

La Layer Normalization est une normalisation interne √† chaque √©chantillon qui stabilise et acc√©l√®re l‚Äôentra√Ænement des mod√®les profonds, sans d√©pendre de la taille du batch.
Elle est indispensable dans les architectures modernes de LLM pour garantir des gradients stables et des performances optimales.

--------------------------------------------------------------------------------------------------------------

## LoRA

LoRA - Low-Rank Adaptation

### ‚öôÔ∏è Qu‚Äôest-ce que LoRA ?

LoRA est une technique de fine-tuning qui permet d‚Äôadapter un LLM sans devoir modifier tous ses param√®tres.
Elle **introduit de petites matrices d‚Äôadaptation √† faible rang dans certaines couches** (souvent les couches d‚Äôattention), tout en laissant les poids originaux du mod√®le gel√©s (non modifi√©s).

### üß† Pourquoi c‚Äôest utile ?

- **√âconomie de m√©moire** : LoRA r√©duit drastiquement le nombre de param√®tres √† entra√Æner.
- **Vitesse** : moins de calculs, donc fine-tuning plus rapide.
- **Modularit√©** : les adaptations LoRA peuvent √™tre sauvegard√©es et r√©utilis√©es ind√©pendamment du mod√®le de base.

### üîß Comment √ßa marche ? - LoRA

1. On d√©compose une matrice de poids (W) en deux petites matrices (A) et (B) de rang r√©duit : Œî(W)=(A)(B)
2. On ins√®re cette adaptation dans le calcul du mod√®le : (W)adapt√© = (W) + Œî(W)
3. On entra√Æne uniquement (A) et (B), pas (W).

### üì¶ Cas d‚Äôusage - LoRA

- Adapter un mod√®le LLM √† un domaine sp√©cifique (juridique, m√©dical, etc.).
- Cr√©er des variantes l√©g√®res et personnalis√©es du m√™me mod√®le de base.
- Fusionner plusieurs LoRA pour obtenir un comportement combin√©.

### ‚úÖ En r√©sum√© - LoRa

Un **LoRA** permet d‚Äôaffiner un LLM en ajoutant des petites matrices adaptatives sans toucher aux poids d‚Äôorigine.
Cela rend l‚Äôadaptation plus l√©g√®re, plus rapide et plus flexible ‚Äî un outil puissant pour d√©ployer des IA personnalis√©es √† grande √©chelle.

--------------------------------------------------------------------------------------------------------------

## Loss function

La **fonction de perte** (loss function) est essentiel √† l‚Äôapprentissage des mod√®les d‚ÄôIA.

### üéØ D√©finition

La fonction de perte mesure l‚Äô**√©cart entre la sortie pr√©dite par le mod√®le et la v√©ritable sortie attendue** (la ‚Äúv√©rit√© terrain‚Äù).
Elle fournit une valeur num√©rique qui guide l‚Äôapprentissage : plus cette valeur est √©lev√©e, plus l‚Äôerreur est grande.

### üß† R√¥le dans l‚Äôapprentissage

Lors de l‚Äôentra√Ænement :

1. Le mod√®le fait une pr√©diction.
2. La fonction de perte calcule l‚Äôerreur.
3. Cette erreur est utilis√©e pour ajuster les poids via la descente de gradient.

### üì¶ Exemples courants

- **MSE (Mean Squared Error)** : erreurs quadratiques moyennes (utilis√©e en r√©gression).
- **Cross-entropy** : mesure la distance entre deux distributions de probabilit√© (utilis√©e en classification, y compris dans les LLM).
- **KL divergence** : √©value la diff√©rence entre une distribution pr√©dite et une distribution cible.

### ‚úÖ En r√©sum√© - Loss function

La fonction de perte est la boussole du mod√®le pendant l‚Äôapprentissage : elle lui indique dans quelle direction corriger ses param√®tres pour s‚Äôam√©liorer.
Sans elle, il n‚Äôy aurait aucun signal d‚Äôapprentissage.

--------------------------------------------------------------------------------------------------------------

## GAN

GAN - Generative Adversarial Network

### üé≠ Qu‚Äôest-ce qu‚Äôun GAN ?

Un **GAN** est un mod√®le g√©n√©ratif constitu√© de deux r√©seaux en comp√©tition :

- Le **g√©n√©rateur** (G) qui apprend √† produire de nouvelles donn√©es (images, sons, etc.) √† partir d‚Äôun bruit al√©atoire.
- Le **discriminateur** (D) qui apprend √† distinguer les donn√©es r√©elles (venant du dataset) des donn√©es synth√©tiques (provenant du g√©n√©rateur).

### üîÑ M√©canisme d‚Äôentra√Ænement

- **Phase 1** : le g√©n√©rateur produit un √©chantillon.
- **Phase 2** : le discriminateur √©value cet √©chantillon et donne un score de ¬´ r√©alit√© ¬ª.
- **Phase 3** :
  - On met √† jour D pour qu‚Äôil am√©liore sa capacit√© √† ne pas se faire duper.
  - On met √† jour G pour qu‚Äôil produise des √©chantillons de plus en plus proches des vraies donn√©es, c‚Äôest-√†-dire qui trompent D.

Cette **adversarial training** (entra√Ænement antagoniste) se poursuit jusqu‚Äô√† ce que G produise des donn√©es suffisamment r√©alistes pour ¬´‚Äâcasser‚Äâ¬ª D la majorit√© du temps.

### üé® Pourquoi c‚Äôest important ?

- **G√©n√©ration haute qualit√©** : les GANs sont capables de cr√©er des images photor√©alistes, de transformer des styles (style transfer) ou de compl√©ter des zones manquantes.
- **Flexibilit√©** : on peut conditionner G pour g√©n√©rer selon des attributs (visages souriants, objets sp√©cifiques‚Ä¶).
- **Applications vari√©es** : art g√©n√©ratif, data augmentation, super-r√©solution d‚Äôimages, simulation de donn√©es m√©dicales‚Ä¶

### ‚úÖ En r√©sum√© - GAN

Les GANs exploitent une dynamique comp√©titive entre un g√©n√©rateur et un discriminateur pour apprendre √† synth√©tiser des donn√©es r√©alistes.

Ce duel interne pousse le g√©n√©rateur √† sans cesse perfectionner ses cr√©ations, ouvrant la voie √† des avanc√©es spectaculaires en g√©n√©ration de contenus.

--------------------------------------------------------------------------------------------------------------

## Gradient descend

Le concept de gradient descent (descente de gradient) est l‚Äôun des piliers de l‚Äôapprentissage en machine learning.

### ‚õ∞Ô∏è Qu‚Äôest-ce que le gradient descent ?

La descente de gradient est un algorithme d‚Äôoptimisation utilis√© pour ajuster les param√®tres d‚Äôun mod√®le afin de **minimiser une fonction de perte**.
Cette **fonction mesure l‚Äôerreur entre les pr√©dictions du mod√®le et les r√©sultats attendus**.

### üßÆ Comment √ßa fonctionne ?

1. Le mod√®le fait une **pr√©diction** √† partir de ses param√®tres actuels.
2. On calcule la **fonction de perte**.
3. On √©value le gradient de cette perte par rapport √† chaque param√®tre (c‚Äôest la direction dans laquelle la perte augmente le plus vite).
4. On met √† jour chaque param√®tre dans la direction oppos√©e √† ce gradient, avec une petite "marche" (le learning rate).

Ce processus est r√©p√©t√© sur de nombreux exemples, jusqu‚Äô√† ce que la perte soit suffisamment faible ou stable.

### üõ†Ô∏è Variantes principales

- **Batch gradient descent** : on utilise tout le dataset √† chaque mise √† jour (lent).
- **Stochastic gradient descent (SGD)** : on met √† jour √† chaque exemple (bruyant, rapide).
- **Mini-batch SGD** : compromis courant, avec des petits groupes d‚Äôexemples.

### ‚úÖ En r√©sum√© - Gadient descent

La descente de gradient est le moteur d‚Äôapprentissage de presque tous les mod√®les modernes d‚ÄôIA. C‚Äôest elle qui ajuste les milliards de param√®tres d‚Äôun LLM comme GPT pour qu‚Äôil devienne progressivement plus performant.

--------------------------------------------------------------------------------------------------------------

## Latent space

Le concept de **latent space** ou **espace latent**, un pilier dans de nombreux mod√®les g√©n√©ratifs et d‚Äôapprentissage profond.

### üåå Qu‚Äôest-ce que l‚Äôespace latent ?

L‚Äôespace latent est un **espace math√©matique** (souvent de dimension r√©duite) **dans lequel les donn√©es r√©elles** (images, textes, sons...) **sont projet√©es sous une forme compress√©e mais significative**.
C‚Äôest une repr√©sentation abstraite o√π chaque point encode des caract√©ristiques importantes, mais non directement observables.

### üß† √Ä quoi √ßa sert ?

L‚Äôid√©e est de r√©sumer l‚Äôinformation : plut√¥t que de manipuler des donn√©es brutes tr√®s complexes, on travaille sur une version compacte qui conserve l‚Äôessentiel.

Exemples :

- En NLP, un mot ou une phrase peut √™tre repr√©sent√© par un vecteur dans l‚Äôespace latent.
- En vision, une image est transform√©e en vecteur qui encode le type d‚Äôobjet, sa position, etc.

### üß© Utilisation typique

- Dans les **autoencodeurs**, l‚Äôencodeur projette les donn√©es dans un espace latent, le d√©codeur les reconstruit √† partir de l√†.
- Les **GANs** et **VAEs** g√©n√®rent de nouvelles donn√©es en √©chantillonnant des points dans l‚Äôespace latent.

Les LLMs utilisent implicitement un espace latent pour comprendre et g√©n√©rer du langage.

### ‚úÖ En r√©sum√© - Latent space

**L‚Äôespace latent est un monde math√©matique invisible dans lequel l‚ÄôIA apprend √† repr√©senter et manipuler les id√©es, les formes et les concepts au-del√† des donn√©es brutes**.

--------------------------------------------------------------------------------------------------------------

## Latent space vs search space

Le **search space** (espace de recherche) et l‚Äôespace latent sont deux notions proches en ce qu‚Äôils d√©crivent tous deux des ‚Äúespaces‚Äù math√©matiques o√π l‚Äôon va chercher des solutions, mais ils servent des objectifs diff√©rents :

### 1. D√©finition du search space

- C‚Äôest l‚Äôensemble **explicite** de **toutes les solutions possibles qu‚Äôun algorithme peut explorer pour r√©soudre un probl√®me donn√©**.
- Par exemple, dans l‚Äôoptimisation d‚Äôhyperparam√®tres d‚Äôun mod√®le, le search space peut √™tre :
  - le choix de l‚Äôapprentissage (n) dans [10‚àí5,10‚àí1],
  - le nombre de couches dans {2,3,4,5},
  - le type de fonction d‚Äôactivation dans {(\text{ReLU, tanh, sigmoid}}}.
- Les m√©thodes de recherche (grid search, random search, Bayesian optimization‚Ä¶) parcourent cet espace pour trouver la combinaison qui minimise la fonction de perte.

### 2. Comparaison avec l‚Äôespace latent

|               | Espace latent                                    | Search space                                    |
|---------------|--------------------------------------------------|--------------------------------------------------|
| **Nature**    | **Continu**, g√©n√©ralement √† dimension r√©duite, appris automatiquement par un mod√®le. | **D√©fini √† la main**, peut √™tre discret, continu ou mixte, selon la t√¢che. |
| **But**       | Encoder la structure sous-jacente des donn√©es pour la reconstruction ou la g√©n√©ration. | Explorer toutes les configurations possibles pour optimiser un crit√®re (perte, score, etc.). |
| **Exploration**  | On √©chantillonne souvent de fa√ßon al√©atoire (VAEs) ou guid√©e (GANs) pour g√©n√©rer de nouvelles donn√©es. | On parcourt syst√©matiquement ou de fa√ßon intelligente pour trouver une configuration optimale. |

#### Exemple - Espace latent

- Vecteurs de dimension 128 o√π chaque point code un visage dans un VAE.
- Repr√©sentation compression d‚Äôune phrase dans un autoencodeur.

#### Exemple - Search space

- Grille de valeurs d‚Äôapprentissage et de r√©gularisation pour entra√Æner un r√©seau.
- Arbre de d√©cisions : ensemble de toutes les structures d‚Äôarbre possibles.

### 3. Quelle diff√©rence ?

- Dans les deux cas on ‚Äúcherche‚Äù quelque chose dans un espace math√©matique.
- Mais l‚Äô**espace latent** est avant tout une **construction interne**, apprise par le r√©seau pour repr√©senter les donn√©es efficacement.
- Tandis que le **search space** est une **enveloppe externe**, d√©finie par l‚Äôutilisateur ou le concepteur de l‚Äôalgorithme pour guider l‚Äôoptimisation.

### ‚úÖ En r√©sum√© - Latent space vs search space

- **Espace latent** : lieu abstrait appris, o√π chaque point code un √©l√©ment de donn√©es (images, textes‚Ä¶) de fa√ßon compress√©e et structur√©e.
- **Search space** : ensemble des solutions potentielles (hyperparam√®tres, architectures‚Ä¶), d√©fini √† l‚Äôavance, que l‚Äôon explore pour optimiser les performances.

--------------------------------------------------------------------------------------------------------------

## Learning-rate

Le ¬´ taux d‚Äôapprentissage ¬ª (ou learning rate en anglais) est un param√®tre cl√© lors de l‚Äôentra√Ænement d‚Äôun mod√®le de machine learning. **Il d√©termine √† quelle vitesse le mod√®le ajuste ses poids en r√©ponse aux erreurs qu‚Äôil commet sur les exemples du jeu de donn√©es**.

### üß† Pourquoi utiliser un taux d‚Äôapprentissage faible lors du fine-tuning ?

Quand on commence avec un mod√®le d√©j√† pr√©-entra√Æn√©, ce mod√®le a d√©j√† appris beaucoup de choses utiles √† partir de grandes quantit√©s de donn√©es g√©n√©rales.

Si on utilisait un taux d‚Äôapprentissage √©lev√©, les ajustements apport√©s aux poids pourraient √™tre trop importants, risquant d‚Äôeffacer ou de d√©stabiliser tout ce que le mod√®le a appris auparavant.
Ce ph√©nom√®ne est parfois appel√© **catastrophic forgetting** (oubli catastrophique) : le mod√®le ¬´ oublie ¬ª ses connaissances initiales parce qu‚Äôon l‚Äôa trop modifi√©.

En utilisant un taux d‚Äôapprentissage faible, on ajuste les poids du mod√®le de mani√®re tr√®s graduelle.
Cela permet d‚Äôintroduire de nouvelles connaissances ou de sp√©cialiser le mod√®le sur un domaine sp√©cifique, sans perturber les connaissances d√©j√† acquises.

On pourrait comparer cela √† un musicien qui, apr√®s des ann√©es √† ma√Ætriser son instrument, apprend √† jouer un nouveau style musical : il n‚Äôa pas besoin de tout r√©apprendre depuis z√©ro, mais plut√¥t d‚Äôapporter des ajustements subtils √† sa technique existante.

--------------------------------------------------------------------------------------------------------------

## MLP

Le **MLP** (Multilayer Perceptron) est une architecture de base dans le deep learning.

### üß± Qu‚Äôest-ce qu‚Äôun MLP ?

Un **MLP** est un **r√©seau de neurones enti√®rement connect√© compos√© de plusieurs couches lin√©aires (dense layers), intercal√©es avec des fonctions d‚Äôactivation non lin√©aires**.
C‚Äôest l‚Äôarch√©type des r√©seaux **feedforward**.

### üß† Structure typique - MLP

- **Input layer** : re√ßoit les donn√©es (ex. : vecteur de caract√©ristiques).
- **Hidden layers** : plusieurs couches de neurones (chaque neurone √©tant connect√© √† tous ceux de la couche pr√©c√©dente), suivies d‚Äôactivations comme ReLU ou Tanh.
- **Output layer** : produit la sortie finale, souvent une classification ou une r√©gression.

### üîç Usage - MLP

- Mod√®les de base pour la **classification**, la **r√©gression**, ou comme composants dans des architectures plus complexes (transformers, autoencodeurs).
- En NLP, un MLP est souvent utilis√© apr√®s **les blocs d‚Äôattention**, dans la partie feedforward d‚Äôun transformer.

### ‚úÖ En r√©sum√© - MLP

Le MLP est une structure simple mais puissante, compos√©e de plusieurs couches de neurones denses.
Elle permet de mod√©liser des fonctions complexes et sert de brique de base dans de nombreuses architectures d‚ÄôIA modernes.

--------------------------------------------------------------------------------------------------------------

## Multi-head-attention

### üîç Qu‚Äôest-ce que la multi-head attention ?

**Dans un mod√®le transformer, l‚Äôattention permet au mod√®le de se concentrer sur diff√©rentes parties du texte en fonction du contexte.**

La multi-head attention consiste √† ex√©cuter plusieurs "attentions" en parall√®le, chacune avec des poids diff√©rents, pour capturer plusieurs types de relations entre les mots.

### üß† Pourquoi plusieurs t√™tes ?

Chaque t√™te d‚Äôattention peut apprendre une relation diff√©rente :

- Une t√™te peut apprendre √† d√©tecter les relations grammaticales.
- Une autre √† comprendre le contexte th√©matique.
- Une autre encore √† suivre des relations temporelles ou de co-r√©f√©rence.

En combinant toutes ces perspectives, le mod√®le obtient une compr√©hension plus fine du texte.

### ‚öôÔ∏è multi-head-attention : Comment √ßa fonctionne ?

- Le texte est projet√© en plusieurs sous-espaces vectoriels.
- Une attention est calcul√©e dans chaque espace (chaque t√™te).
- Les r√©sultats sont concat√©n√©s et transform√©s pour produire la sortie.

### ‚úÖ En r√©sum√© - Multi-head-attention

La multi-head attention permet √† un mod√®le de regarder un texte sous diff√©rents angles simultan√©ment, ce qui am√©liore la richesse de l‚Äôanalyse contextuelle.
C‚Äôest l‚Äôun des piliers de la puissance des LLM modernes.

--------------------------------------------------------------------------------------------------------------

## Prompt engineering

### üîç Qu‚Äôest-ce que le prompt engineering ?

Le prompt engineering est l‚Äôart de concevoir des instructions (ou prompts) efficaces et cibl√©es pour interagir avec un mod√®le de langage comme GPT.
Ces mod√®les sont sensibles √† la formulation des requ√™tes, donc bien structurer une question ou un contexte peut grandement influencer la qualit√© et la pertinence de la r√©ponse g√©n√©r√©e.

### üß† Pourquoi c‚Äôest important ? - Prompt engineering

M√™me si un mod√®le est tr√®s performant, il peut donner des r√©ponses impr√©cises ou hors sujet si le prompt est vague ou mal formul√©.
En affinant le prompt, on peut :

- guider le mod√®le vers un certain style de r√©ponse,
- limiter les hallucinations,
- obtenir des r√©ponses plus concises, factuelles ou adapt√©es au contexte.

#### üß™ Exemples

- Prompt vague : ¬´ Explique l‚ÄôIA. ¬ª
- Prompt efficace : ¬´ En une phrase simple, explique √† un enfant de 10 ans ce qu‚Äôest l‚Äôintelligence artificielle. ¬ª

### ‚úÖ Utilisations

Le prompt engineering est devenu une comp√©tence cl√© pour :

- les d√©veloppeurs d‚Äôapplications IA,
- les chercheurs,
- les cr√©ateurs de contenu,
- les √©quipes UX cherchant √† exploiter les LLM de mani√®re fiable et contr√¥l√©e.

C‚Äôest une technique puissante et accessible qui permet d‚Äôexploiter tout le potentiel des LLM sans n√©cessiter de modifier le mod√®le lui-m√™me.

--------------------------------------------------------------------------------------------------------------

## Sampling

### üéØ Qu‚Äôest-ce que le sampling ?

Le sampling (√©chantillonnage) est le **processus** par lequel **un mod√®le de langage choisit le ou les prochains tokens √† g√©n√©rer**, √† partir des probabilit√©s qu‚Äôil a calcul√©es.
C‚Äôest une √©tape cruciale pour passer d‚Äôune distribution pr√©dite √† un texte concret.

### üß† Pourquoi ne pas simplement prendre le token le plus probable ?

Toujours choisir le token le plus probable (greedy decoding) rend les textes :

- r√©p√©titifs,
- peu cr√©atifs,
- parfois incoh√©rents √† long terme.

Le sampling introduit de la variabilit√©, ce qui permet :

- des r√©ponses plus naturelles,
- une meilleure couverture des possibles,
- une adaptation au ton ou au style d√©sir√©.

### üî¢ Techniques courantes

- **Top-k sampling** : on **garde seulement les k tokens les plus probables**, puis on choisit au **hasard** parmi eux.
- **Top-p sampling** (aussi appel√© nucleus sampling) : on garde les tokens qui cumulent un certain pourcentage p de probabilit√© (ex : 90%) et on choisit parmi eux.
- **Temperature scaling** : on ajuste la "concentration" des probabilit√©s. Une temp√©rature basse (ex : 0.2) rend le mod√®le plus s√ªr et r√©p√©titif, une temp√©rature haute (ex : 1.0 ou plus) le rend plus cr√©atif, mais parfois incoh√©rent.

### ‚úÖ En r√©sum√© - Sampling

Le sampling est le m√©canisme qui transforme les pr√©dictions du mod√®le en texte lisible.
Selon la m√©thode utilis√©e, on peut obtenir des r√©sultats tr√®s d√©terministes ou tr√®s vari√©s, selon les besoins de l‚Äôutilisateur.

--------------------------------------------------------------------------------------------------------------

## Sampling vs Transformer

Le **sampling** n‚Äôest pas un bloc physique int√©gr√© √† l‚Äôarchitecture transformer elle-m√™me, mais plut√¥t une p**roc√©dure de d√©codage qui s‚Äôapplique apr√®s que le transformer ait produit ses scores**.

### 1. Sortie du transformer

1. Le transformer (decoder-only ou encoder-decoder) traite les tokens d‚Äôentr√©e √† travers ses couches d‚Äô**auto-attention** et **feed-forward**.
2. Apr√®s la derni√®re couche, on passe la repr√©sentation interne de chaque position dans une **couche lin√©aire** (projection) qui donne un vecteur de logits de taille ‚à£ùëâ‚à£ (taille du vocabulaire).
3. On applique un softmax sur ces logits pour obtenir une distribution de probabilit√© ùëÉ(token ùëñ ‚à£ contexte).
logits = Wh + b ‚ü∂ P = softmax(logits)

### 2. D√©codage / Sampling

1. D√©codage d√©terministe (greedy, beam search) : on choisit les tokens de plus haute probabilit√©, encha√Æn√©s selon la strat√©gie.
2. √âchantillonnage (sampling) : on tire au hasard un token √† partir de la distribution ùëÉ, √©ventuellement restreinte par top-k, top-p, ou ajust√©e par une temp√©rature.
3. Cette √©tape de sampling est donc externe au r√©seau ; on ne ‚Äúrecalcule‚Äù rien dans le transformer, on se contente de tirer (√©chantillonner) un index selon ùëÉ.

### 3. Pourquoi ce d√©coupage ?

**Modularit√©** : on peut facilement tester plusieurs strat√©gies de g√©n√©ration (greedy, beam, top-k, top-p‚Ä¶) sans toucher aux poids du mod√®le.
**Simplicit√© d‚Äôimpl√©mentation** : le transformer produit des probabilit√©s, et n‚Äôimporte quel code client (API, boucle Python‚Ä¶) peut ensuite appliquer le sampling de la mani√®re souhait√©e.

### ‚úÖ En r√©sum√© - Sampling vs Transformer

Le transformer se charge de produire, pour chaque position, une distribution sur les tokens. Le sampling est un algorithme de d√©codage, ind√©pendant de l‚Äôarchitecture, qui choisit un token √† partir de cette distribution pour g√©n√©rer le texte final.

--------------------------------------------------------------------------------------------------------------

## Tokens

### üî§ Qu‚Äôest-ce qu‚Äôun *token* ?

Un **token** est une unit√© de texte utilis√©e par les mod√®les de langage.
Ce n‚Äôest **pas forc√©ment un mot** : il peut s‚Äôagir d‚Äôun mot entier, d‚Äôun morceau de mot, ou m√™me de signes de ponctuation.
Par exemple :

- ¬´ chat ¬ª = 1 token
- ¬´ internationalisation ¬ª = 3 tokens
- ¬´ ! ¬ª = 1 token

Les LLM ne traitent pas directement des mots, mais des s√©quences de tokens.

### üìè Pourquoi les tokens comptent ?

Chaque mod√®le a une **limite de tokens** qu‚Äôil peut traiter dans une fen√™tre de contexte.

Cette limite influence :

- la longueur maximale d‚Äôune entr√©e ou d‚Äôune r√©ponse,
- les co√ªts (dans les API comme OpenAI, la facturation est bas√©e sur le nombre de tokens),
- les performances (plus de tokens = plus de calculs).

### üß† Tokenization

Avant d‚Äô√™tre trait√©e, une phrase est **tokenis√©e**, c‚Äôest-√†-dire d√©coup√©e en tokens selon un algorithme sp√©cifique (comme *Byte-Pair Encoding* ou *Unigram Language Model*).
Cela permet une analyse fine du texte par le mod√®le.

### ‚úÖ En r√©sum√© - Tokens

Un *token* est l‚Äôunit√© de base du langage pour un LLM.
Comprendre ce qu‚Äôest un token est essentiel pour bien utiliser les mod√®les, √©valuer les co√ªts et optimiser les prompts.

--------------------------------------------------------------------------------------------------------------

## Transformer

### üß† Qu‚Äôest-ce qu‚Äôun transformer ?

Un **transformer est une architecture de r√©seau de neurones** introduite par Google en 2017 dans l'article Attention is All You Need.
Elle a r√©volutionn√© le traitement du langage naturel (NLP) en rempla√ßant les architectures s√©quentielles (comme les RNN ou LSTM) par une approche parall√®le plus efficace.

### ‚öôÔ∏èLes composants cl√©s

- **Embedding** : transforme les mots en vecteurs num√©riques.
- **Positional Encoding**: ajoute de l‚Äôinformation sur la position des mots dans la phrase.
- **Attention mechanism** : permet au mod√®le de pond√©rer l‚Äôimportance de chaque mot du contexte.
- **Multi-head attention** : le mod√®le regarde plusieurs types de relations en parall√®le.
- **Feed-forward layers** : couches denses qui traitent les repr√©sentations obtenues.
- **Layer normalization & r√©sidus** : stabilisent l‚Äôapprentissage.

### ‚öôÔ∏èLes composants cl√©s - v2

- **Embedding** : conversion des tokens en vecteurs.
- **Positional encoding** : ajout d‚Äôune information sur la position des tokens dans la s√©quence (puisque le mod√®le ne traite pas les entr√©es s√©quentiellement).
- **Self-attention** : chaque token ¬´ regarde ¬ª tous les autres pour capturer le contexte global.
- **Feedforward** : chaque token passe par un MLP identique.
- **LayerNorm** + **r√©sidus** : stabilisation et acc√©l√©ration de l‚Äôapprentissage.

### üß† Variants d‚Äôarchitecture

- **Encoder-only** : ex. BERT (analyse de texte).
- **Decoder-only** : ex. GPT (g√©n√©ration de texte).
- **Encoder-decoder** : ex. T5, BART (traduction, r√©sum√©).

### üìö Pourquoi c‚Äôest important ?

Les transformers sont √† la base de mod√®les comme BERT, GPT, T5, et bien d‚Äôautres.
Leur capacit√© √† traiter un texte entier en parall√®le les rend beaucoup plus rapides et puissants que les mod√®les s√©quentiels.

### ‚úÖ En r√©sum√© - Transformer

Le **Transformer** est l‚Äôarchitecture c≈ìur des LLM modernes, avec un fonctionnement fond√© sur l‚Äôattention plut√¥t que sur l‚Äôordre strict des mots.
Cela leur permet de mieux comprendre le contexte global d‚Äôune s√©quence.

Le **Transformer** a r√©volutionn√© l‚ÄôIA en rempla√ßant les r√©seaux s√©quentiels par une architecture parall√©lisable, plus efficace, capable de traiter des s√©quences longues tout en capturant un riche contexte global via l‚Äôattention.
C‚Äôest la colonne vert√©brale de tous les grands mod√®les de langage contemporains.

--------------------------------------------------------------------------------------------------------------

  </textarea>
 
  <div id="content" class="content"></div>


<br>

<hr>

<br>


  </main>

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      const markdownText = document.getElementById('markdown').value.trim();
      const contentDiv = document.getElementById('content');
      contentDiv.innerHTML = marked.parse(markdownText);
    });
  </script>
</body>
</html>
